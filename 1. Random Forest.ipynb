{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It is a ML technique that combines several base models in order to produce one optimal predictive model.\n",
    "M1,M2,M3....,Mn .... we consider result of each n every single model to create the final best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of ensemble method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Bagging\n",
    "   Bootstrapping + Aggregation\n",
    "   process does happen paralelly.\n",
    "2. Boosting\n",
    "   process does happen sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "It is supervised ML algorithm.\n",
    "- based on ensemble method i.e bagging.\n",
    "  Bagging\n",
    "  B - Bootstrapping\n",
    "  agging - Aggregation\n",
    "    \n",
    "  Bootstrapping means we have to use samples with replacement.\n",
    "\n",
    "It pics the observations randomly and it also take the obs. twice or thrice.\n",
    "BD are made randomly picking the values, tht is the reason word random is here.\n",
    "By default it RF create 100 datasets and all will be unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "O  BD\n",
    "1  1\n",
    "2  3\n",
    "3  1\n",
    "4  4\n",
    "5  6 \n",
    "6  3 >>>> OOB >> 2,5 >> These are the out of bag samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "After aggregation from every DT, voting happens for final output/result.\n",
    "1. Hard voting\n",
    "2. Soft voting\n",
    "\n",
    "      0     1       Classified in\n",
    "c1  [0.9   0.1]        class 0\n",
    "c2  [0.3   0.7]        class 1\n",
    "c3  [0.8   0.2]        class 0\n",
    "0,1,0\n",
    "Class 0 >> is our final outcome.\n",
    "\n",
    "\n",
    "p(0) = (0.9 + 0.3 + 0.8)/3 = (2.10)/3 = 0.7\n",
    "p(1) = (0.1 + 0.7 + 0.2)/3 = (1/3) = 0.33\n",
    "\n",
    "P(0) > p(1) >> final outcome is class 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "For regression, whatever result we get from different decision trees, we take the mean/average of them as a final result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. max_features: 'auto' >> np.sqrt\n",
    "    original dataset does hv 100 features\n",
    "    BD >> sqrt(100) >> 10\n",
    "    50 >> 7\n",
    "2. OOB_score:\n",
    "    evaluation metric \n",
    "    where ever we get a good OOB_score, we can say that our model is performing well.\n",
    "    OOB_score = (no. of correctly classified samples)/(Total no. of OOB samples.)\n",
    "   OOB_error \n",
    "    = (no. of incorrectly classified samples)/(Total no. of OOB samples.)\n",
    "    \n",
    "4. criterion\n",
    "5. n_estimator : no. of decision trees >> by default it is 100\n",
    "6. max_depth\n",
    "7. min_samples_split : 2\n",
    "8. min_samples_leaf  : 1\n",
    "9. bootstrap : True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100\n",
    "\n",
    "20/100 >> 0.2\n",
    "70/100 >> 0.7\n",
    ">> 0-1\n",
    "\n",
    "OOB_error >> near to 0\n",
    "OOB_score >> near to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Reduces Overfitting issue\n",
    "2. Simple to implement.\n",
    "3. used for both classification and regression\n",
    "4. feature scaling does not required.\n",
    "5. Works well with large datasets too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disadvantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. It is more complex.\n",
    "2. Require more time in comparison with DT.\n",
    "3. Although it does work well for classification n regression too, but in some cases it does not work well in regression.\n",
    "4. It fails to determine significance of each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. finance\n",
    "2. Marketing\n",
    "3. Medical"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
